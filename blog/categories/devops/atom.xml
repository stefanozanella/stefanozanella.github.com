<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: devops | Stefano Zanella's Blog]]></title>
  <link href="http://blog.dontwakethecat.net/blog/categories/devops/atom.xml" rel="self"/>
  <link href="http://blog.dontwakethecat.net/"/>
  <updated>2013-09-22T18:33:53+02:00</updated>
  <id>http://blog.dontwakethecat.net/</id>
  <author>
    <name><![CDATA[Stefano Zanella]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Speaking in tongues with SSL]]></title>
    <link href="http://blog.dontwakethecat.net/blog/2013/01/22/speaking-in-tongues-with-ssl/"/>
    <updated>2013-01-22T16:56:00+01:00</updated>
    <id>http://blog.dontwakethecat.net/blog/2013/01/22/speaking-in-tongues-with-ssl</id>
    <content type="html"><![CDATA[<p>This was a nice one; let me explain.</p>

<p>I've setup a PotgreSQL host to hold all the various databases for the various
services deployed in the infrastructure. Since I know I'm not a security
expert, where I can I try to do the bare minumum needed and use SSL. This is
the case for PostgreSQL; not only, but I try hard to enforce two-way
certificate validation where possibile.</p>

<!-- More -->


<p>It turns out that despite the simplicity behind the concept of two-way
certificate validation, very few <em>"modern"</em> services support that in a
user-friendly way. I already had a chance to rant a bit on Twitter about the
problems Puppet is currently facing with SSL; this time I want to tell you this
story that involves the <a href="https://code.google.com/p/gerrit/">Gerrit Code Review</a>
application and the way I solved the problem, which in my opinion is quite
hacky (and will possibly break things in a near future).</p>

<p>First, a little overview of two-way SSL certificate validation. Basically, in
normal circumstances, during a SSL handshake, only the client verifies that the
certificate the server is providing is valid (checking against a list of known
and trusted Certificate Authorities); when performing two-way validation, this
process is true also for the server. That is, the server expects the client to
send a certificate and verifies that it can be trusted with the same mechanism.<br/>
This is useful when you're running your own PKI and can freely issue
certificates to all your hosts; it becomes a form of authentication similar to
that we all use when doing public key authentication in SSH (maybe even
stronger).<br/>
For this to work, obviously, there must be explicit configuration support on
the client service to point to certificate, private key and CA that will be
used when handshaking with the server. Also, note that you cannot simply
disable certificate validation, since this will only lower the barrier <strong>on one
side</strong> of the communication channel: the server will still require you (the
client) to provide a valid certificate (in most cases you can tell the server
to relax the constraint, but then this discussion would become a little
pointless).</p>

<p>Here is where problems begin: let's look specifically at how this can be
handled in Gerrit.<br/>
In particular, let's see how a typical database setup looks like in Gerrit:
<code>ini
[database]
  type = POSTGRESQL
  hostname = postgresql.derecom.it
  database = gerrit
  username = gerrit
</code>
This will tell JDBC to connect to <code>postgresql.derecom.it</code> and look for database
<code>gerrit</code>, authenticating with user <code>gerrit</code> (password is handled in another
file). See? No mention to <strong>SSL</strong>. Unofrtunately, JDBC doesn't automatically
recognizes that it needs to setup a SSL connection, and trying to boot the
gerrit server results in this kind of error on the PosgtreSQL host:
<code>
FATAL:  no pg_hba.conf entry for host "x.y.z.w", user "gerrit", database "gerrit", SSL off
</code>
(that's because I <strong>don't allow unencrypted connections to databases</strong>).</p>

<p>It seems that there's no hope to solve this. Luckily, though, Gerrit use JDBC
under the hood to manage the connection pool; looking through
<a href="http://jdbc.postgresql.org/documentation/80/connect.html">JDBC PosgtreSQL driver documentation</a>,
we can read that:</p>

<blockquote><p> In addition to the standard connection parameters the driver supports a
 number of additional properties which can be used to specify additional
 driver behavior specific to PostgreSQL™. These properties may be specified
 in either the connection URL or an additional Properties object parameter to
 DriverManager.getConnection. The following examples illustrate the use of
 both methods to establish a SSL connection.</p>

<p>String url = "jdbc:postgresql://localhost/test";<br/>
Properties props = new Properties();<br/>
props.setProperty("user","fred");<br/>
props.setProperty("password","secret");<br/>
props.setProperty("ssl","true");<br/>
Connection conn = DriverManager.getConnection(url, props);</p>

<p>String url =<br/>
"jdbc:postgresql://localhost/test?user=fred&amp;password=secret<strong>&amp;ssl=true</strong>";<br/>
Connection conn = DriverManager.getConnection(url);</p></blockquote>

<p>So, it seems that if we could pass a <code>ssl</code> parameter to the JDBC URL we could
enable SSL while connection to PostgreSQL. A first solution is to set the
connection type to <code>JDBC</code> instead of <code>POSTGRESQL</code> in Gerrit configuration. This
would allow you to directly specify the URL JDBC should connect to, parameters
included.<br/>
Since I didn't know if I would have had to specify also the user/pass in that
same URL, I wanted to try to stick to the <code>POSTGRESQL</code> type. Looking at
<a href="https://gerrit.googlesource.com/gerrit/+/7029fc15df86e6ef886d67a8117a39d21320fe60/gerrit-pgm/src/main/java/com/google/gerrit/pgm/util/DataSourceProvider.java">this class' source code</a>,
it turns out that Gerrit itself builds a JDBC URL, so using one of the other
connection types is just a tiny wrapper over the JDBC type. However, the URL is
built leaving out username and password, which are set separately when actually
instantiating the connection, and the last variable in the URL concatenation is
the database name. So, I modified the configuration by directly appending the
<code>ssl</code> param to the database name:
<code>ini
[database]
  type = POSTGRESQL
  hostname = postgresql.derecom.it
  database = gerrit?ssl=true
  username = gerrit
</code></p>

<p>You know what? That works like a charm :)<br/>
Obviously this is a bit hacky, and surely setting a JDBC is a more proper way
to handle this thing, however if it works...</p>

<p><strong>PS:</strong> I left out from the discussion a fundamental step, which is passing the
certificate, private key and CA to Gerrit to correctly handle the SSL
handshake. This involves passing two additional properties to Gerrit startup
command; I've already written down about it in
<a href="https://gist.github.com/4124338">this Gist</a>, so I won't repeat myself here.</p>

<h2>Conclusion</h2>

<p>So, having spent almost 2 hours fixing this issue and writing about it, what I
can hope for is that developers start to care a little more about SSL and the
various ways it can be used. This would surely help changing the impression
that SSL is a though beast, as also noticed by
<a href="https://twitter.com/_masterzen_">Brice Figureau</a> in
<a href="http://www.masterzen.fr/2010/11/14/puppet-ssl-explained/">this post about Puppet SSL PKI</a>, and making our
infrastructures more secure overall.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Master of Puppets]]></title>
    <link href="http://blog.dontwakethecat.net/blog/2013/01/20/master-of-puppets/"/>
    <updated>2013-01-20T01:16:00+01:00</updated>
    <id>http://blog.dontwakethecat.net/blog/2013/01/20/master-of-puppets</id>
    <content type="html"><![CDATA[<p>Let me start the real first post of this blog (apart from the introduction)
with a citation. Despite the title, I won't talk about music. Instead, I'll
show you a photo:</p>

<!-- More -->


<p><img src="http://farm9.staticflickr.com/8330/8395925627_7fd19c4aba_z.jpg"></p>

<p>It's a shot I've taken in <strong>Bern</strong> on August 2011; it's the window of a toys
shop you can find under the cloisters of Gerechtigkeitstraße. Its name? <em>Antics
und Puppenklinik</em>, which even without translation projects us directly to the
focus of this and probably subsequent writings.</p>

<p>As promised in my <a href="http://blog.dontwakethecat.net/blog/2013/01/19/pre-flight-debrief/">first post</a>
I'll document the setup of a node that will act as the <strong>puppetmaster</strong> for my
company's network. If you don't know what I'm talking about, you probably
landed on the wrong page.<br/>
If, however, you're a sysadmin or an operations guy
and still don't know what I'm talking about, you should <strong>IMMEDIATELY</strong> go
checking out <a href="http://docs.puppetlabs.com/">Puppet Documentation</a> on the
<a href="http://puppetlabs.com/">Puppet Labs website</a>.</p>

<p>Before starting, here's a brief overview. We'll start by installing a fresh
<strong>CentOS 6.3</strong> on a <strong>KVM</strong> virtual machine, then we will step into installing
<strong>Puppet</strong> and configuring it so it can start talking with other machines. For
sure, it isn't rocket science; but again, I'm just trying to document the most
I can about what I do.</p>

<h2>Installing Bare OS</h2>

<p>Select OS for the task is, as preannounced, <strong>CentOS</strong>. This is my standard
choice where no special features are needed (think multimedia packages, for the
most part). The reason of this choice is not fully clear even to me, but I
think my preference goes to it for its small footprint, the availability of
third-party repositories with fairly updated, yet robust, packages that makes
it in general quite stable, even when upgrading. On this subject, I remember
painful hours spent on distro/packages broken upgrade paths with Debian and
derivatives; I'm sure things got better in the last few years, but for the
moment I see little to no reason to move to another OS as my standard choice.</p>

<p>The puppetmaster will run on a VM hosted on a KVM hypervisor; I can hear you
say: <em>"Why not cloud?"</em>. I can say there's a reason, which is associated
mainly to costs, but I'll leave that discussion for another post.</p>

<p>To install VMs on KVM/libvirt I usually rely on <strong>virt-install</strong>, a Python tool
that takes care of generating the relevant XML definition for the VM and booting
it with an installation media that it's removed when installation finishes.<br/>
On CentOS, it can be installed on the hypervisor simply with:
<code>bash
yum install python-virtinst
</code></p>

<p>I won't go through here on what you need to have a working KVM setup: the only
thing I need to point out for the moment is that I have configured a pool in
libvirt associated to a <strong>LVM Volume Group</strong> called <code>vmstorage</code>.</p>

<p>That said, let's start the installation process. We'll create a VM with <strong>1GB
RAM</strong>, <strong>1 virtual CPU</strong> and <strong>10GB of storage</strong>. You can download the CentOS
installation CD .iso from
<a href="http://mi.mirror.garr.it/mirrors/CentOS/6.3/isos/x86_64/CentOS-6.3-x86_64-minimal.iso">here</a>.
I suggest downloading the minimal installation pack since it requires much less
space than the full DVD and provides everything needed to install a bare
minimum system.
Once you have saved the installation media, say, into <code>/media</code>, we can proceed
with booting the VM:
<code>bash
virt-install --name=bernstein --ram=1024 --vcpus=1 \
--cdrom=/media/CentOS-6.3-x86_64-minimal.iso \
--os-type=linux --os-variant=rhel6 \
--disk pool=vmstorage,size=10 --network bridge=br0,model=e1000 \
--video=vga --vnc --connect qemu:///system
</code>
At this point we don't have any textual access to the VM; however, we can
setup a SSH tunnel to the hypervisor binding the port where the VNC server
for the instance is listening, which can be found with:
<code>bash
virsh vncdisplay bernstein
:3
</code>
After that, if you are working on a Mac, i suggest you to try the excellent
<a href="http://sourceforge.net/projects/chicken/">Chicken</a> VNC client (I've
tried many, even the Screen Sharing app available by default, but found this is
the app that works best). The latest version (<strong>2.2b2</strong> as time of writing)
can also automatically setup a SSH tunnel for you.</p>

<p>After we have a connection to the VM's display, we can start the installation
process. To fully document it, I'd need to share a screenshot for every step.
Since I'm not a Martian, you will excuse me if I go through this phase by
simply pointing out the relevant configuration options.
Here are the values of the fields filled during installation:</p>

<ul>
<li><strong>Language</strong>: English</li>
<li><strong>Keyboard layout</strong>: U.S. English</li>
<li><strong>Device Type</strong>: Basic Storage Devices</li>
<li><strong>Hostname</strong>: bernstein.derecom.it</li>
<li><strong>Timezone</strong>: Europe/Rome</li>
<li><strong>Root password</strong>: Hahaha, really you thought I'd have told you?</li>
<li><strong>Disk partitioning</strong>: Select <em>"Replace Existing Linux Systems"</em>, then tick
<em>Review and modify partitioning layout</em>. Modify default layout as follows:

<ul>
<li>/dev/vda1   500   ext4    /boot</li>
<li>/dev/vda2   9739  LVM     os

<ul>
<li>root      7720  ext4    /root</li>
<li>swap      2016  swap    -</li>
</ul>
</li>
</ul>
</li>
</ul>


<p>After the installation process finishes, the VM will be shut down. If we want
to restart it and also have it automatically restarted when server reboots, we
can issue the following commands:
<code>bash
virsh autostart bernstein
virsh start bernstein
</code>
After that, we still need to connect via VNC before we'll be able to SSH into
the machine. Here are the basic steps needed to setup networking (I do not use
DHCP on the main server's subnet):
<code>bash
vi /etc/sysconfig/network-scripts/ifcfg-eth0
</code>
<code>
DEVICE="eth0"
BOOTPROTO="static"
HWADDR="52:54:00:77:21:AE"
NM_CONTROLLED="no"
ONBOOT="yes"
TYPE="Ethernet"
UUID="8af5d6eb-e0c3-4401-9fd9-cad42c011d3b"
IPADDR=172.16.32.253
NETMASK=255.255.255.0
GATEWAY=172.16.32.254
DNS1=8.8.8.8
</code>
<code>bash
service network restart
</code>
Also, I setup some mnemonic entries on our authoritative DNS. They will come in
handy very soon:
<code>
bernstein               A       172.16.32.253
puppet                  CNAME   bernstein
puppetmaster            CNAME   bernstein
</code></p>

<p>At this point, we can start do some basic configuration before installing
Puppet.</p>

<h2>Basic Configuration</h2>

<p>First thing first: disable SELinux. I haven't found the time yet to study and
understand how SELinux works, so I still prefer to let it out of the game:
<code>bash
sed -i -e 's/SELINUX=enforcing/SELINUX=permissive/g' /etc/sysconfig/selinux
setenforce permissive # This way we avoid to reboot
</code></p>

<p>Then, I usually install two additional repositories before doing anything else:
<a href="http://fedoraproject.org/wiki/EPEL">EPEL</a> and
<a href="http://repoforge.org/">RepoForge (was RPMForge)</a>:
<code>bash
rpm -Uvh http://ftp.upjs.sk/pub/mirrors/epel/6/x86_64/epel-release-6-8.noarch.rpm
rpm -Uvh http://pkgs.repoforge.org/rpmforge-release/rpmforge-release-0.5.2-2.el6.rf.x86_64.rpm
</code>
Then it's good to update available software:
<code>bash
yum update
</code>
The next step regards NTP and shouldn't be needed, but in my case I found it necessary.<br/>
In fact, in virtualized environments clock sync should be provided directly by
hypervisor by exposing an already NTP synced RTC. This was actually the case
when I used to use Xen, and it still works like that in KVM; the only problem
is that the clock still presents jiffies. So, the best thing to do is install
NTP and forget about that:
<code>bash
yum install ntp
chkconfig ntpd on
</code>
Before starting the service, we force an initial synchronization with:
<code>bash
ntpd -q
</code>
Then:
<code>bash
service ntpd start
</code>
Till now, I never encountered problems with the provided defaults. Maybe in the future
I'll also dig into this aspect and customize the configuration; for the moment
I'll leave as it is.</p>

<p>This ends the list of basic configuration tasks I usually do. We can now
stop to shave the yak and proceed to dig into the actual subject of the post.</p>

<h2>Installing Puppet</h2>

<p>The awesome guys at <a href="http://puppetlabs.com">Puppet Labs</a> provide the right
repositories for the job; we can just add them and perform a one-command Puppet
installation:
<code>bash
rpm -Uvh http://yum.puppetlabs.com/el/6/products/x86_64/puppetlabs-release-6-6.noarch.rpm
</code>
If you want to dig more into the topic, here's a
<a href="http://docs.puppetlabs.com/guides/puppetlabs_package_repositories.html">very good section</a>
on the Puppet Labs documentation site.</p>

<p>After adding the repo, Puppet server can be installed with:
<code>bash
yum install puppet-server
</code>
This will also take care of installing Ruby. The latest version available in
CentOS is still a 1.8.7 as time of writing, but it's perfectly suitable to run
Puppet's applications.</p>

<h2>Configuring the Master</h2>

<p>We want to configure the puppetmaster both as the master as well as an agent
for itself. Both configurations are done in <code>/etc/puppet/puppet.conf</code>.<br/>
We start by configuring the most basic options for both the master and the
agent; then we'll move into configuring the right SSL support for our
infrastructure.</p>

<h3>Basic configuration options</h3>

<p>For what concerns the master, the only option we touch for the moment is the
one that enables distribution of custom facts and types from the server to the
agents (plugin sync):
<code>ini
[main]
...
pluginsync = true
...
</code>
This same option, since it is set in the <code>main</code> section, also apply for the
agent.</p>

<p>Also, for the agent, we set the appropriate name for the master via the
<code>server</code> directive. By default the agent would look for an host named <code>puppet</code>;
since our master will be reachable via its FQDN, we need to be explicit about
this. This option belongs to the <code>agent</code> section:
<code>ini
[agent]
...
server = puppet.derecom.it
...
</code></p>

<h3>SSL Setup</h3>

<p><em>I lost almost an afternoon writing about my intended SSL setup, just to
discover that as of current version (3.0.2), Puppet pose serious limits on
swapping its internal PKI management. In my head, I wanted to completely
disable it and provide certificates myself from my already working PKI.<br/>
Unfortunately, there's a serious incompatibility in how OpenSSL creates
certificates subjects and what Puppet intends as a "valid" subject. If you
want, you can read more <a href="http://projects.puppetlabs.com/issues/15561">here</a>.<br/>
Until the issue is resolved, I'm forced to rely on default PKI management; for
this to work, no additional configuration steps are needed.</em></p>

<h3>Service Setup</h3>

<p>Now we're finally ready to start the master. With everything in place, it's no harder than:
<code>bash
service puppetmaster start
</code></p>

<p>Similarly, for the agent:
<code>bash
chkconfig puppet on
service puppet start
</code></p>

<p>Notice that we don't enable the provided service for the master at boot. This is
because we'll setup a proxy with Nginx and Passenger as the following task. We
just run it once so the master can create its own PKI, which is needed in order
to accomplish the following section.</p>

<h3>Proxying Master with Nginx + Passenger</h3>

<p>As suggested by
<a href="http://docs.puppetlabs.com/guides/installation.html#post-install">official documentation</a>,
the default WEBrick server is not suitable for real-life workloads. So, here
we'll setup the master to receive requests from a proxy instead of directly
handling them. Selected stack is <a href="http://httpd.apache.org/">Apache</a> and
<a href="http://www.modrails.com/">Phusion Passenger</a>. This is because the version of
nginx that can be found into Passenger or EPEL repos doesn't ship with the
<code>ngx_headers_more</code> module, needed to set additional headers when processing
client requests.
The procedure depicted here is adapted from the relative
<a href="http://docs.puppetlabs.com/guides/passenger.html">article</a> in Puppet's
documentation.</p>

<p>We start by adding the Phusion Passenger repository, which provides an updated
version of Passenger itself, as well as some other goodies we don't need ATM:
<code>bash
yum install http://passenger.stealthymonkeys.com/rhel/6/passenger-release.noarch.rpm
</code></p>

<p>Then we install the relevant applications:
<code>bash
yum install httpd mod_passenger mod_ssl
</code></p>

<p>At this point we need to setup a root for the Rack application that will serve
Puppet requests. A default Rack application expects three files:</p>

<ul>
<li>a <code>config.ru</code> which can be called by Rack itself</li>
<li>a <code>public/</code> folder</li>
<li>a <code>tmp/</code> folder</li>
</ul>


<p>We start with the required folders, setting our root at <code>/opt/puppetmaster</code>:
<code>bash
mkdir -p /opt/puppetmaster/{tmp,public}
</code></p>

<p>Thankfully, Puppet Labs ships a working <code>config.ru</code> file into the master's
package:
<code>bash
cp /usr/share/puppet/ext/rack/files/config.ru /opt/puppetmaster/
chown puppet:puppet /opt/puppetmaster/config.ru
</code></p>

<p>The only thins that's left to do is to setup the required Apache virtual host.
Again, Puppet Labs provides a sample configuration file that can be used as a
base:
<code>bash
cp /usr/share/puppet/ext/rack/files/apache2.conf /etc/httpd/conf.d/puppetmaster.conf
</code></p>

<p>The content of the file must be edited to reflect actual paths as follows:
```</p>

<h1>Tunable settings</h1>

<p>PassengerHighPerformance on
PassengerMaxPoolSize 12
PassengerPoolIdleTime 1500</p>

<h1>PassengerMaxRequests 1000</h1>

<p>PassengerStatThrottleRate 120
RackAutoDetect Off
RailsAutoDetect Off</p>

<p>Listen 8140</p>

<p><VirtualHost *:8140></p>

<pre><code>    SSLEngine on
    SSLProtocol -ALL +SSLv3 +TLSv1
    SSLCipherSuite ALL:!ADH:RC4+RSA:+HIGH:+MEDIUM:-LOW:-SSLv2:-EXP

    SSLCertificateFile
</code></pre>

<p>/var/lib/puppet/ssl/certs/bernstein.derecom.it.pem</p>

<pre><code>    SSLCertificateKeyFile
</code></pre>

<p>/var/lib/puppet/ssl/private_keys/bernstein.derecom.it.pem</p>

<pre><code>    SSLCertificateChainFile /var/lib/puppet/ssl/certs/ca.pem
    SSLCACertificateFile    /var/lib/puppet/ssl/certs/ca.pem
    SSLCARevocationFile     /var/lib/puppet/ssl/crl.pem
    SSLVerifyClient optional
    SSLVerifyDepth  1
    # The `ExportCertData` option is needed for agent certificate
</code></pre>

<p>expiration warnings</p>

<pre><code>    SSLOptions +StdEnvVars +ExportCertData

    # This header needs to be set if using a loadbalancer or proxy
    RequestHeader unset X-Forwarded-For

    RequestHeader set X-SSL-Subject %{SSL_CLIENT_S_DN}e
    RequestHeader set X-Client-DN %{SSL_CLIENT_S_DN}e
    RequestHeader set X-Client-Verify %{SSL_CLIENT_VERIFY}e

    DocumentRoot /opt/puppetmaster/public/
    RackBaseURI /
    &lt;Directory /opt/puppetmaster/&gt;
            Options None
            AllowOverride None
            Order allow,deny
            allow from all
    &lt;/Directory&gt;
</code></pre>

<p></VirtualHost>
```</p>

<p>At this point, we just need to enable Apache at boot and replace the running
puppetmaster daemon:
<code>bash
chkconfig httpd on
service puppetmaster stop
service httpd start
</code></p>

<h3>Firewall setup</h3>

<p>Before rolling, we need to setup iptables and open the relevant port in order
for the master to be reachable:
<code>bash
vi /etc/sysconfig/iptables
</code>
<code>
...
-A INPUT -m state --state NEW -m tcp -p tcp --dport 8140 -j ACCEPT
...
</code>
<code>bash
service iptables restart
</code></p>

<h2>Testing the installation</h2>

<p>A quick test can be done on the master itself by running a one-shot agent, like
this:
<code>bash
puppet agent --test --debug
</code></p>

<p>If everything is working as expected, the last non-debug line should report a
notice that catalog run went fine:
<code>
Notice: Finished catalog run in 0.04 seconds
</code></p>

<p>Obviously, the master doesn't need to sign the certificate, since the agent is
picking the same certificate the master created on the first run. For the other
agents, we'll need to manually sign certificates when new CSRs arrive.</p>

<h2>To be continued...</h2>

<p>I wrap this post here, since it's already too long, but I'll continue in the
next one with an important task: putting Puppet's configuration under version
control and activating a (continuous) deployment pipeline in Jenkins for it.<br/>
I promise that will be more intersting than this post ;-)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Pre-flight debrief]]></title>
    <link href="http://blog.dontwakethecat.net/blog/2013/01/19/pre-flight-debrief/"/>
    <updated>2013-01-19T20:20:00+01:00</updated>
    <id>http://blog.dontwakethecat.net/blog/2013/01/19/pre-flight-debrief</id>
    <content type="html"><![CDATA[<p>I'm writing this post as a sort of disclamer for the posts that will follow in
the (hopefully near) future.</p>

<p>I became a fan of documentation near three years ago. Since then, I mantained
(and am still mantaining) a sort of private wiki where I try do document
whatever I do. In some cases, this practice saved my ass: since for the moment
I don't work in a team, I'm forced to continuously move between tasks that span
from the full Dev and Ops range. Sometimes quite some time can pass until I
need to return to a previous task to fix/improve things. This can be daunting,
since memory tends to be pretty unreliable in these cases; having a neat trace
of what I did helped me to quickly restore the archived knowledge.<br/>
Over time, documenting <strong>while</strong> doing has become almost a natural practice for
me, at least for what concerns operations tasks.</p>

<p>As stated above, till now I kept this documentation almost private, just
visible to interested friends that wanted to dig from time to time into what I
did; now I feel this practice has to change.<br/>
So, I decided to at least try to make my efforts public. I'm sure there won't
be crowds of developers or sysadmins waiting for my next post; I'm not doing it
for this reason. I'll just try to use the lever that is what in my opinion is
the biggest benfit of open source culture: it forces you to be better at what
you do.</p>

<p>So, this little detour over my recent history just to say one basic thing: I'm
gonna store here almost everything. So, expect to find obvious things as well
as not-so-obvious ones; actually, I suspect the former will overweight the latter
at least for the moment :)</p>

<p>Having clarified this basic concept, let's see what the sudden future reserves:
I'm gonna talk a lot in the following months about my efforts to automate the
infrastructure I'm actually managing. This effort started slightly more than a
year ago when I discovered the excellent <a href="http://continuousdelivery.com/">"Continuous Delivery" book</a>
by <a href="https://twitter.com/jezhumble">Jez Humble</a> and David Farley.
Meanwhile, I've done a lot to try to add some solid
ground on the infrastructure while maintaining the existent. I now reached a
point in which I can start to implement all those nice automation practices.<br/>
I'll try to convert documentation I've already written into something suitable
for the public audience; but, since I don't want to start yak shaving right
from the beginning, I'll try to stay pragmatic enough and start by writing
currently on-air tasks. For this reason, I apologize in advance if some pieces
will miss from what I'll write.</p>

<p>I force myself now to stop adding to this post; enough self-centered writing
for my tastes. Better to start writing something more practical; I'll start
with something very very basic and in very very little DevOps style: installing
a Puppet master node.</p>
]]></content>
  </entry>
  
</feed>
